{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2faa24",
   "metadata": {},
   "source": [
    "# LXMERT: Learning Cross-Modality Encoder Representations from Transformers\n",
    "### By Mulla Meharaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46eca9d",
   "metadata": {},
   "source": [
    "### Overview\n",
    "LXMERT, which stands for **Learning Cross-Modality Encoder Representations from Transformers**, is pretty cool in how it brings together **visual content** from images and **natural language** from text. I really like this deep learning framework, especially for stuff like **Visual Question Answering (VQA)**. It does a great job of mixing visual and textual info, making everything work together smoothly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c68605",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "Traditional models process vision and language separately, which limits their ability to handle tasks requiring joint understanding—like answering “What is the person doing in the image?” These tasks demand integrated reasoning across both modalities.\n",
    "\n",
    "LXMERT addresses this by learning joint image-text representations through a Transformer-based architecture. It combines a language encoder, an object-relationship encoder, and a cross-modality encoder to align visual and linguistic features.\n",
    "\n",
    "Pre-trained on over 9 million image-sentence pairs using tasks like masked language modeling, object prediction, and image question answering, LXMERT captures both intra-modality and cross-modality relationships. This results in state-of-the-art performance on benchmarks such as VQA, GQA, and NLVR2, showcasing its strength in vision-language reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb80d1",
   "metadata": {},
   "source": [
    "###  Model Architecture\n",
    "####  Encoders in LXMERT:\n",
    "- **Language Encoder**: Based on a Transformer architecture, it processes tokenized text inputs using WordPiece embeddings and positional encodings. It captures contextual word representations using multiple layers of self-attention and feed-forward sub-layers, following the BERT-style design.\n",
    "- **Object-Relationship Encoder**:  Instead of raw pixel data, images are represented as a sequence of object features extracted via a pre-trained Faster R-CNN detector. Each object is embedded with both its visual (RoI) features and spatial (bounding box) coordinates, allowing the encoder to model inter-object relationships using self-attention layers.\n",
    "- **Cross-Modality Encoder**: The core of LXMERT, this encoder fuses visual and textual information through bi-directional cross-attention—from language to vision and vice versa—followed by self-attention within each modality. This enables rich, aligned representations that capture interactions between specific words and image regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb91421",
   "metadata": {},
   "source": [
    "Image -> Object Features |\n",
    "Text -> Token Features |\n",
    "Both fed into parallel encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2b399",
   "metadata": {},
   "source": [
    "![Model Diagram](model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c99268",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "#### Text Input (Word-Level)\n",
    "- WordPiece Tokenizer: As described, LXMERT uses the WordPiece tokenizer, which splits input text into a series of tokens. This is particularly beneficial for handling rare words by breaking them down into more common subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7df30",
   "metadata": {},
   "source": [
    "#### Embedding Process\n",
    "- Token Embeddings: After tokenization, each token is transformed into a continuous representation (embedding).\n",
    "- Positional Embeddings: Positional information is crucial because the model needs to understand the order of words in a sentence. Each token position in the sentence is embedded as a position vector.\n",
    "- Combination: The token embeddings and positional embeddings are summed to produce the final word-level sentence embeddings. This combination allows the model to interpret both the identity and order of the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1cbca",
   "metadata": {},
   "source": [
    "#### Image Input (Object-Level)\n",
    "- Feature Extraction with Faster R-CNN: LXMERT uses a pre-trained Faster R-CNN model to detect objects within the image. For each detected object, the model extracts a Region-of-Interest (RoI) feature vector, which serves as a compact representation of the object's appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c26bb",
   "metadata": {},
   "source": [
    "#### Embedding Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00016f",
   "metadata": {},
   "source": [
    "- RoI Features: These features are direct outputs from the Faster R-CNN and represent the visual characteristics of the objects detected in the image.\n",
    "- Position Embeddings: In addition to visual features, spatial information from bounding box coordinates (like x, y positions of the bounding box, width, and height) is converted into position embeddings.\n",
    "- Combination: The RoI features and position embeddings are combined to form the final object-level embeddings. This results in embeddings that encapsulate not just what the object looks like, but where it is located within the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f7115",
   "metadata": {},
   "source": [
    "### Attention Layers\n",
    "- **Self-Attention**: Models internal relationships (within text or image).\n",
    "- **Cross-Attention**: Models inter-modal relationships (between image and text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64050f91",
   "metadata": {},
   "source": [
    "Attention: Query → Key, Value\n",
    "Cross-attention: Text ↔ Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea82589",
   "metadata": {},
   "source": [
    "### Pretraining Tasks\n",
    "LXMERT is pre-trained on **5 core tasks** to build robust cross-modal understanding:\n",
    "\n",
    "1. **Masked Language Modeling** (like BERT but cross-modal)\n",
    "2. **Masked Object Prediction**\n",
    "   - Feature regression (RoI features)\n",
    "   - Label classification (object class)\n",
    "3. **Cross-Modality Matching**\n",
    "4. **Image Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f10ec2",
   "metadata": {},
   "source": [
    "For example: Predict masked word/object using vision+language context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e7f24",
   "metadata": {},
   "source": [
    "![Pretraining Tasks](preTraining.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfcc4e",
   "metadata": {},
   "source": [
    "### Datasets Used for Pretraining\n",
    "- **MS COCO**, **Visual Genome**\n",
    "- QA datasets: **VQA**, **GQA**, **VG-QA**\n",
    "- Total: **~9.18 Million** image-text pairs, **180K** images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2040a6",
   "metadata": {},
   "source": [
    "### Evaluation Highlights\n",
    "| Dataset | Metric | LXMERT Performance |\n",
    "|--------|--------|--------------------|\n",
    "| VQA    | Accuracy | 72.5% (SOTA)       |\n",
    "| GQA    | Accuracy | 60.3%              |\n",
    "| NLVR2  | Accuracy | 76.2%              |\n",
    "\n",
    "LXMERT significantly outperforms prior models, especially on complex reasoning datasets like **NLVR2** (improvement of 22% absolute!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53cc38a",
   "metadata": {},
   "source": [
    "### Summary\n",
    "LXMERT (Learning Cross-Modality Encoder Representations from Transformers) is a foundational model designed to bridge the gap between vision and language in AI. It leverages the power of Transformers, a state-of-the-art architecture in sequence modeling, to build a multi-modal understanding of image-text pairs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
